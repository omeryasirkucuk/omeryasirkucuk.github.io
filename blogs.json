{"status":"ok","feed":{"url":"https://medium.com/feed/@omeryasirkucuk","title":"Stories by \u00d6mer Yasir K\u00fc\u00e7\u00fck on Medium","link":"https://medium.com/@omeryasirkucuk?source=rss-6e7a0ab6f65d------2","author":"","description":"Stories by \u00d6mer Yasir K\u00fc\u00e7\u00fck on Medium","image":"https://cdn-images-1.medium.com/fit/c/150/150/1*G6QRbKjrgHikLhc37Ait7g.jpeg"},"items":[{"title":"Notes to me: Different Join Types (Natural &amp; Lateral Join)","pubDate":"2024-12-22 05:45:03","link":"https://omeryasirkucuk.medium.com/notes-to-me-different-join-types-natural-lateral-join-639a204aba5c?source=rss-6e7a0ab6f65d------2","guid":"https://medium.com/p/639a204aba5c","author":"\u00d6mer Yasir K\u00fc\u00e7\u00fck","thumbnail":"","description":"\n<p>When working with databases, you often need to combine data from different tables. Two fascinating ways to do this are <strong>Natural Joins</strong> and <strong>Lateral Joins</strong>. Although their names might sound intimidating, these concepts are straightforward and incredibly useful when applied correctly. Let\u2019s explore what makes them\u00a0unique.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*y5WKRT6fvt0FoaF9LY4GGw.png\"><figcaption>Source: <a href=\"https://blog.replaybird.com/postgresql-lateral-join/\">https://blog.replaybird.com/postgresql-lateral-join/</a></figcaption></figure><blockquote>Lateral</blockquote>\n<p><strong>Lateral Join</strong> is more advanced and dynamic join type. It allows the right-hand table (or subquery) <em>to reference</em> columns from the left-hand table. This capability is invaluable for queries requiring row-wise calculations or nested lookups. In summary, we can easily use our main table in subquery.</p>\n<pre>SELECT <br>  o.OrderID, <br>  o.OrderDate,<br>  p.ProductName<br>FROM <br>  Orders o, <br>LEFT JOIN LATERAL (<br>    SELECT <br>      ProductName<br>    FROM <br>      Products<br>    WHERE 1=1<br>      AND Products.OrderID = o.OrderID<br>) p ON true;</pre>\n<pre>SELECT <br>  o.OrderID, o.OrderDate, p.ProductName<br>FROM <br>  Orders o, <br>LATERAL (<br>    SELECT <br>      ProductName<br>    FROM <br>      Products<br>    WHERE 1=1<br>      AND Products.OrderID = o.OrderID<br>) p;</pre>\n<p>While PostgreSQL and Oracle directly support the \u201cLATERAL\u201d keyword; \u201cCROSS APPLY\u201d serves the same purpose in SQL Server. It is also included in DWH solutions such as Snowflake.</p>\n<blockquote>Natural Join</blockquote>\n<p>When you\u2019re tired of writing out all those matching columns for your joins, a <strong>Natural Join</strong> comes to the rescue. It <em>automatically </em>matches columns between two tables that share the same name and have compatible data types. This way, you can skip specifying the join condition, making your query cleaner\u200a\u2014\u200aespecially when tables are designed with consistent naming conventions.</p>\n<pre>---- Without Natural Join<br>SELECT <br>  o.OrderID, <br>  o.OrderDate,<br>  p.ProductName<br>FROM <br>  Orders o, <br>LEFT JOIN Products p ON p.OrderID = o.OrderID<br>;</pre>\n<pre>---- With Natural Join<br>SELECT <br>  o.OrderID, <br>  o.OrderDate,<br>  p.ProductName<br>FROM <br>  Orders o, <br>NATURAL LEFT JOIN Products<br>;</pre>\n<p>We don\u2019t need to specify ON parameters in joins. But it could be little risky in terms of limited control in join criteria and unintended results if tables have columns with the same name but unrelated meanings.</p>\n<h4>Source:</h4>\n<p><a href=\"https://docs.snowflake.com/en/sql-reference/constructs/join\">https://docs.snowflake.com/en/sql-reference/constructs/join</a></p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=639a204aba5c\" width=\"1\" height=\"1\" alt=\"\">\n","content":"\n<p>When working with databases, you often need to combine data from different tables. Two fascinating ways to do this are <strong>Natural Joins</strong> and <strong>Lateral Joins</strong>. Although their names might sound intimidating, these concepts are straightforward and incredibly useful when applied correctly. Let\u2019s explore what makes them\u00a0unique.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*y5WKRT6fvt0FoaF9LY4GGw.png\"><figcaption>Source: <a href=\"https://blog.replaybird.com/postgresql-lateral-join/\">https://blog.replaybird.com/postgresql-lateral-join/</a></figcaption></figure><blockquote>Lateral</blockquote>\n<p><strong>Lateral Join</strong> is more advanced and dynamic join type. It allows the right-hand table (or subquery) <em>to reference</em> columns from the left-hand table. This capability is invaluable for queries requiring row-wise calculations or nested lookups. In summary, we can easily use our main table in subquery.</p>\n<pre>SELECT <br>  o.OrderID, <br>  o.OrderDate,<br>  p.ProductName<br>FROM <br>  Orders o, <br>LEFT JOIN LATERAL (<br>    SELECT <br>      ProductName<br>    FROM <br>      Products<br>    WHERE 1=1<br>      AND Products.OrderID = o.OrderID<br>) p ON true;</pre>\n<pre>SELECT <br>  o.OrderID, o.OrderDate, p.ProductName<br>FROM <br>  Orders o, <br>LATERAL (<br>    SELECT <br>      ProductName<br>    FROM <br>      Products<br>    WHERE 1=1<br>      AND Products.OrderID = o.OrderID<br>) p;</pre>\n<p>While PostgreSQL and Oracle directly support the \u201cLATERAL\u201d keyword; \u201cCROSS APPLY\u201d serves the same purpose in SQL Server. It is also included in DWH solutions such as Snowflake.</p>\n<blockquote>Natural Join</blockquote>\n<p>When you\u2019re tired of writing out all those matching columns for your joins, a <strong>Natural Join</strong> comes to the rescue. It <em>automatically </em>matches columns between two tables that share the same name and have compatible data types. This way, you can skip specifying the join condition, making your query cleaner\u200a\u2014\u200aespecially when tables are designed with consistent naming conventions.</p>\n<pre>---- Without Natural Join<br>SELECT <br>  o.OrderID, <br>  o.OrderDate,<br>  p.ProductName<br>FROM <br>  Orders o, <br>LEFT JOIN Products p ON p.OrderID = o.OrderID<br>;</pre>\n<pre>---- With Natural Join<br>SELECT <br>  o.OrderID, <br>  o.OrderDate,<br>  p.ProductName<br>FROM <br>  Orders o, <br>NATURAL LEFT JOIN Products<br>;</pre>\n<p>We don\u2019t need to specify ON parameters in joins. But it could be little risky in terms of limited control in join criteria and unintended results if tables have columns with the same name but unrelated meanings.</p>\n<h4>Source:</h4>\n<p><a href=\"https://docs.snowflake.com/en/sql-reference/constructs/join\">https://docs.snowflake.com/en/sql-reference/constructs/join</a></p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=639a204aba5c\" width=\"1\" height=\"1\" alt=\"\">\n","enclosure":{},"categories":["sql","join"]},{"title":"NiFi, ElasticSearch &amp; Kibana Data Pipeline on Docker by consuming S3 Data","pubDate":"2024-04-23 19:44:13","link":"https://omeryasirkucuk.medium.com/nifi-elasticsearch-kibana-data-pipeline-on-docker-by-consuming-s3-data-ba09563a5319?source=rss-6e7a0ab6f65d------2","guid":"https://medium.com/p/ba09563a5319","author":"\u00d6mer Yasir K\u00fc\u00e7\u00fck","thumbnail":"","description":"\n<p>Unlocking the power of data integration and visualization has never been more accessible. In this guide, we delve into the seamless orchestration of NiFi, ElasticSearch, and Kibana within Docker, providing a robust data pipeline solution. By harnessing the capabilities of these tools and consuming S3 data, we embark on a journey to streamline data processing and visualization, empowering organizations to extract actionable insights with\u00a0ease.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*tNYGZjBJybe7ytUq90Eo_A.png\"></figure><blockquote><strong>NiFi Installation on\u00a0Docker:</strong></blockquote>\n<p>Since we will create NiFi, ElasticSearch and Kibana in different containers, we want them all to be in a communicating network. That\u2019s why we create a\u00a0network:</p>\n<pre>docker network create elastic</pre>\n<p>Dockerfiles of the technologies we will use have already been uploaded to DockerHub by the developers. It is possible to work by collecting all of these in a compose file. In this implementation we will proceed using the latest versions available on DockerHub.</p>\n<pre>docker pull apache/nifi</pre>\n<p>While running the container, we connect the network and port and give the detach command to run in the background:</p>\n<pre>docker run --name devnifi --net elastic -p 8443:8443 -e NIFI_WEB_HTTPS_PORT='8443' -d apache/nifi:latest</pre>\n<p>We are establishing a registry together with NiFi. NiFi Registry is used alongside NiFi to manage, version, and share data flows efficiently, providing version control and collaboration capabilities. It facilitates seamless distribution of data flows across different environments while ensuring security and monitoring through authentication and authorization features.</p>\n<pre>docker pull apache/nifi-registry<br>docker run --name nifi-registry --net elastic -p 18080:18080 -d apache/nifi-registry</pre>\n<p>To access NiFi and NiFi registry:</p>\n<pre>https://localhost:8443/nifi<br>http://localhost:18080/nifi-registry</pre>\n<p>NiFi will ask for username and password when logging in. We can find this information by looking at the logs of our container. To look at the logs, you can click on your container from the Container tab in the Docker application, or you can first find the id of your container by typing docker ps in the Terminal and then type docker logs id to go into\u00a0details.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/406/1*RFqECOY6gXgQhYf90jxqRg.png\"><figcaption>Example username and password:</figcaption></figure><p>Before moving on to using NiFi, let\u2019s set up ElasticSearch and Kibana with\u00a0Docker</p>\n<blockquote>ElasticSearch Installation on\u00a0Docker:</blockquote>\n<pre>docker pull docker.elastic.co/elasticsearch/elasticsearch:7.17.20<br><br>docker run --name es01-test -d --net elastic -p 9200:9200 -p 9300:9300 -e \"discovery.type=single-node\" docker.elastic.co/elasticsearch/elasticsearch:7.17.20<br># for security option: docker run --name es01-test -d --net elastic -p 9200:9200 -p 9300:9300 -e \"ELASTIC_PASSWORD=omer\" -e \"discovery.type=single-node\" docker.elastic.co/elasticsearch/elasticsearch:7.17.20<br># after that go to elastic container, files, /usr/share/elasticsearch/config/elasticsearch.yml and write: <br># xpack.security.enabled: true<br># xpack.security.authc.api_key.enabled: true<br># and for kibana: /usr/share/kibana/config/kibana.yml and write: <br>elasticsearch.username: \"elastic\"<br>elasticsearch.password: \"omer\"</pre>\n<blockquote>Kibana Installation on\u00a0Docker:</blockquote>\n<pre>docker pull docker.elastic.co/kibana/kibana:7.17.20<br>docker run --name kib01-test -d --net elastic -p 127.0.0.1:5601:5601 -e \"ELASTICSEARCH_HOSTS=http://es01-test:9200\" docker.elastic.co/kibana/kibana:7.17.20</pre>\n<blockquote>To access ElasticSearch and\u00a0Kibana:</blockquote>\n<pre>http://localhost:9200/ #elasticsearch details<br>http://localhost:5601/ #kibana interface</pre>\n<p>Important Note:</p>\n<p><em>In the links given, all ports are given according to the installation. If you used different port settings, please change the links accordingly.</em></p>\n<p><em>Additionally, if you want to automatically save the data you create in these containers to your local, you can solve this problem by creating a\u00a0volume.</em></p>\n<h4>Pipeline Phase on NiFi with data on AWS S3\u00a0Bucket:</h4>\n<p>When the NiFi screen opens, a blank page will greet us. It may make sense to use Processer Group to perform complex and independent work.</p>\n<p>NiFi works with the drag and drop method. Let\u2019s start by creating a new Processer Group.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/558/1*wu7Ilm1zoLZFC_ZlMWgLhQ.png\"><figcaption>After dragging and dropping the Processer Group icon, we give it a name. I gave a name it <strong>test</strong> as an example. Then, we open this group by double-clicking on\u00a0it.</figcaption></figure><p>After entering the processor we prepared, we need to add the necessary Processor to read the data from the S3 Bucket. Let\u2019s take a quick look at this\u00a0screen:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*m72g3nIEgxaPrgUZ3N-51g.png\"><figcaption>It is possible to process data from many sources with the processors on this screen. In the screenshot I am filtering only S3. I will use the ListS3 processor to connect to my\u00a0bucket.</figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*8MZkhuCXHBJLt9AZYEVL8w.png\"><figcaption>ListS3 Config</figcaption></figure><p>We fill in the bucket, region, access key id, secret access key and prefix fields in the ListS3 processor.</p>\n<p>If your S3 bucket is <strong>not public</strong> and your IAM user does not have <strong>read permission</strong> to your S3 bucket, you may receive an error\u00a0here.</p>\n<p>Currently, we just did something to list the data in our bucket. We use the FetchS3Object processor to get the data\u00a0here:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/904/1*SJ9P0swYo_Pja8QUvreh_w.png\"><figcaption>We enter the bucket name, region and access keys into FetchS3Object. Afterwards, we move the mouse cursor over ListS3, hold the arrow that appears, move it to FetchS3Object and release it, creating a flow that will run in case of \u201csuccess\u201d.</figcaption></figure><p>To start the flow, we can right-click on each processor and click Run once or Start. In order for the entire flow to run at the same time, we can press the Start button from the Operate section on the left of the\u00a0screen.</p>\n<p>ElasticSearch does not accept data in any form. There are some limitations. For this reason, we need to prepare the data.<br>For this, I add ConvertRecord, SplitJson and QueryRecord processors respectively and connect them together.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*IyIgLev7wekFtRzOkE-eaw.png\"><figcaption>ConvertRecord Details:<br>If this is your first time preparing a datapipeline in NiFi, this is probably new to you. After double-clicking on the ConvertRecord you dragged and dropped, we need to set the Record Reader and Record Writer sections in the Properties section. This is a system for us to manage the data that this processor receives and gives. First, let\u2019s set up Record Reader one by one, as in the screenshot. With Create new service, we will create and activate a service suitable for our data\u00a0type.</figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/992/1*MpzhWIio9z-0HTdSiSWcDQ.png\"><figcaption>Since I will be reading a JSON file, I specify the service as JsonTreeReader. For Record Writer, we specify JsonRecordSetWriter.</figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*YL_a4LwMK9jZmIp6mBfTuA.png\"><figcaption>We created these services, but we did not activate them. After clicking the arrow in the image, we activate it by pressing the <strong>lightning</strong> bolt on the screen that\u00a0appears.</figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*0brp2Gz1MmGKyyMIaR4kDQ.png\"><figcaption>In case of failure in these processors, we drag the arrow on the processor and drop it back on itself to be\u00a0queued.</figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*m7ZRSTPHWvsSKJCWXLfXZw.png\"><figcaption>We can see the data in Success by running the steps we created one by one. To do this, we can see the data when we right-click and click on \u201clist\u00a0queue\u201d.</figcaption></figure><p>In SplitJson, we write $ into JsonPath Expression. Afterwards, we put the QueryRecord processor and create the Record Writer and Reader for JSON again and give them a different name. Output Grouping \u201cOne Line Per Object\u201d must be selected in the JsonRecordSetWriter settings for\u00a0Reader.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*HSNmAIIW9jPVDc6suspk8w.png\"><figcaption>We determine the details of our new data by clicking \u201cAdd Property\u201d in QueryRecord. We give the names of all variables and finally add the time when the data was processed with ${now():toNumber()} as timestamp. Adding timestamp is optional.</figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*9LyVXaRY-EFk7d1iWE1mSQ.png\"><figcaption>Our last action in Pipeline is to send the data to ElasticSearch. We use the PutElasticsearchHttp processor for this. For this, we need to provide the Elasticsearch URL.<br>The critical thing here is to give the correct path. Since we are running all services with Docker, this path will be local.<br>In the Index section in this field, we write the index name to which this data will be assigned in ElasticSearch.</figcaption></figure><p>Important Note:</p>\n<p><em>If a \u201cConnection Refused\u201d error is received in the localhost:9200 or 127.0.0.1:9200 urls, go to Terminal and type \u201c</em><strong><em>hostname\u201d</em></strong><em> and the result should be written instead of localhost or 127.0.0.1.</em></p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*lCibLwod5RHZB28gbKahLA.png\"><figcaption>After these operations, we run each processor and finish the flow. We put a Funnel at the end of the stream. Let\u2019s start ElasticSearch and Kibana operations by going to the URL <a href=\"http://127.0.0.1:5601/\">http://127.0.0.1:5601</a>.</figcaption></figure><blockquote>Visualize Your Data with\u00a0Kibana</blockquote>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/502/1*Ms2UMuBit48c0EblUaxa5Q.png\"><figcaption>We see the index we sent with NiFi by entering Index Management in the Stock Management tab.</figcaption></figure><p>To visualize the data in this index, we enter Index Patterns under Kibana in the Stock Management tab and create a new\u00a0pattern.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*KNaFoDJf7mwpeIcbcE_Rgg.png\"><figcaption>Afterwards, we enter the Dashboard from the Analytics section and use the data as we wish. This is the part that is left to your imagination\u00a0:)</figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*C47iD25tL7fKEs21oK9K5Q.png\"></figure><blockquote>Sources:</blockquote>\n<p><a href=\"https://hub.docker.com/_/kibana\">https://hub.docker.com/_/kibana</a></p>\n<p><a href=\"https://www.linkedin.com/pulse/installing-apache-nifi-ubuntu-2004-lts-aws-dhruv-sahu/\">https://www.linkedin.com/pulse/installing-apache-nifi-ubuntu-2004-lts-aws-dhruv-sahu/</a></p>\n<p><a href=\"https://www.youtube.com/watch?v=U8dNNYeiD9o\">https://www.youtube.com/watch?v=U8dNNYeiD9o</a></p>\n<p><a href=\"https://www.youtube.com/watch?v=QXjcLenjz0k\">https://www.youtube.com/watch?v=QXjcLenjz0k</a></p>\n<p><a href=\"https://hub.docker.com/_/elasticsearch\">https://hub.docker.com/_/elasticsearch</a></p>\n<p><a href=\"https://hub.docker.com/r/apache/nifi\">https://hub.docker.com/r/apache/nifi</a></p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=ba09563a5319\" width=\"1\" height=\"1\" alt=\"\">\n","content":"\n<p>Unlocking the power of data integration and visualization has never been more accessible. In this guide, we delve into the seamless orchestration of NiFi, ElasticSearch, and Kibana within Docker, providing a robust data pipeline solution. By harnessing the capabilities of these tools and consuming S3 data, we embark on a journey to streamline data processing and visualization, empowering organizations to extract actionable insights with\u00a0ease.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*tNYGZjBJybe7ytUq90Eo_A.png\"></figure><blockquote><strong>NiFi Installation on\u00a0Docker:</strong></blockquote>\n<p>Since we will create NiFi, ElasticSearch and Kibana in different containers, we want them all to be in a communicating network. That\u2019s why we create a\u00a0network:</p>\n<pre>docker network create elastic</pre>\n<p>Dockerfiles of the technologies we will use have already been uploaded to DockerHub by the developers. It is possible to work by collecting all of these in a compose file. In this implementation we will proceed using the latest versions available on DockerHub.</p>\n<pre>docker pull apache/nifi</pre>\n<p>While running the container, we connect the network and port and give the detach command to run in the background:</p>\n<pre>docker run --name devnifi --net elastic -p 8443:8443 -e NIFI_WEB_HTTPS_PORT='8443' -d apache/nifi:latest</pre>\n<p>We are establishing a registry together with NiFi. NiFi Registry is used alongside NiFi to manage, version, and share data flows efficiently, providing version control and collaboration capabilities. It facilitates seamless distribution of data flows across different environments while ensuring security and monitoring through authentication and authorization features.</p>\n<pre>docker pull apache/nifi-registry<br>docker run --name nifi-registry --net elastic -p 18080:18080 -d apache/nifi-registry</pre>\n<p>To access NiFi and NiFi registry:</p>\n<pre>https://localhost:8443/nifi<br>http://localhost:18080/nifi-registry</pre>\n<p>NiFi will ask for username and password when logging in. We can find this information by looking at the logs of our container. To look at the logs, you can click on your container from the Container tab in the Docker application, or you can first find the id of your container by typing docker ps in the Terminal and then type docker logs id to go into\u00a0details.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/406/1*RFqECOY6gXgQhYf90jxqRg.png\"><figcaption>Example username and password:</figcaption></figure><p>Before moving on to using NiFi, let\u2019s set up ElasticSearch and Kibana with\u00a0Docker</p>\n<blockquote>ElasticSearch Installation on\u00a0Docker:</blockquote>\n<pre>docker pull docker.elastic.co/elasticsearch/elasticsearch:7.17.20<br><br>docker run --name es01-test -d --net elastic -p 9200:9200 -p 9300:9300 -e \"discovery.type=single-node\" docker.elastic.co/elasticsearch/elasticsearch:7.17.20<br># for security option: docker run --name es01-test -d --net elastic -p 9200:9200 -p 9300:9300 -e \"ELASTIC_PASSWORD=omer\" -e \"discovery.type=single-node\" docker.elastic.co/elasticsearch/elasticsearch:7.17.20<br># after that go to elastic container, files, /usr/share/elasticsearch/config/elasticsearch.yml and write: <br># xpack.security.enabled: true<br># xpack.security.authc.api_key.enabled: true<br># and for kibana: /usr/share/kibana/config/kibana.yml and write: <br>elasticsearch.username: \"elastic\"<br>elasticsearch.password: \"omer\"</pre>\n<blockquote>Kibana Installation on\u00a0Docker:</blockquote>\n<pre>docker pull docker.elastic.co/kibana/kibana:7.17.20<br>docker run --name kib01-test -d --net elastic -p 127.0.0.1:5601:5601 -e \"ELASTICSEARCH_HOSTS=http://es01-test:9200\" docker.elastic.co/kibana/kibana:7.17.20</pre>\n<blockquote>To access ElasticSearch and\u00a0Kibana:</blockquote>\n<pre>http://localhost:9200/ #elasticsearch details<br>http://localhost:5601/ #kibana interface</pre>\n<p>Important Note:</p>\n<p><em>In the links given, all ports are given according to the installation. If you used different port settings, please change the links accordingly.</em></p>\n<p><em>Additionally, if you want to automatically save the data you create in these containers to your local, you can solve this problem by creating a\u00a0volume.</em></p>\n<h4>Pipeline Phase on NiFi with data on AWS S3\u00a0Bucket:</h4>\n<p>When the NiFi screen opens, a blank page will greet us. It may make sense to use Processer Group to perform complex and independent work.</p>\n<p>NiFi works with the drag and drop method. Let\u2019s start by creating a new Processer Group.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/558/1*wu7Ilm1zoLZFC_ZlMWgLhQ.png\"><figcaption>After dragging and dropping the Processer Group icon, we give it a name. I gave a name it <strong>test</strong> as an example. Then, we open this group by double-clicking on\u00a0it.</figcaption></figure><p>After entering the processor we prepared, we need to add the necessary Processor to read the data from the S3 Bucket. Let\u2019s take a quick look at this\u00a0screen:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*m72g3nIEgxaPrgUZ3N-51g.png\"><figcaption>It is possible to process data from many sources with the processors on this screen. In the screenshot I am filtering only S3. I will use the ListS3 processor to connect to my\u00a0bucket.</figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*8MZkhuCXHBJLt9AZYEVL8w.png\"><figcaption>ListS3 Config</figcaption></figure><p>We fill in the bucket, region, access key id, secret access key and prefix fields in the ListS3 processor.</p>\n<p>If your S3 bucket is <strong>not public</strong> and your IAM user does not have <strong>read permission</strong> to your S3 bucket, you may receive an error\u00a0here.</p>\n<p>Currently, we just did something to list the data in our bucket. We use the FetchS3Object processor to get the data\u00a0here:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/904/1*SJ9P0swYo_Pja8QUvreh_w.png\"><figcaption>We enter the bucket name, region and access keys into FetchS3Object. Afterwards, we move the mouse cursor over ListS3, hold the arrow that appears, move it to FetchS3Object and release it, creating a flow that will run in case of \u201csuccess\u201d.</figcaption></figure><p>To start the flow, we can right-click on each processor and click Run once or Start. In order for the entire flow to run at the same time, we can press the Start button from the Operate section on the left of the\u00a0screen.</p>\n<p>ElasticSearch does not accept data in any form. There are some limitations. For this reason, we need to prepare the data.<br>For this, I add ConvertRecord, SplitJson and QueryRecord processors respectively and connect them together.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*IyIgLev7wekFtRzOkE-eaw.png\"><figcaption>ConvertRecord Details:<br>If this is your first time preparing a datapipeline in NiFi, this is probably new to you. After double-clicking on the ConvertRecord you dragged and dropped, we need to set the Record Reader and Record Writer sections in the Properties section. This is a system for us to manage the data that this processor receives and gives. First, let\u2019s set up Record Reader one by one, as in the screenshot. With Create new service, we will create and activate a service suitable for our data\u00a0type.</figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/992/1*MpzhWIio9z-0HTdSiSWcDQ.png\"><figcaption>Since I will be reading a JSON file, I specify the service as JsonTreeReader. For Record Writer, we specify JsonRecordSetWriter.</figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*YL_a4LwMK9jZmIp6mBfTuA.png\"><figcaption>We created these services, but we did not activate them. After clicking the arrow in the image, we activate it by pressing the <strong>lightning</strong> bolt on the screen that\u00a0appears.</figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*0brp2Gz1MmGKyyMIaR4kDQ.png\"><figcaption>In case of failure in these processors, we drag the arrow on the processor and drop it back on itself to be\u00a0queued.</figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*m7ZRSTPHWvsSKJCWXLfXZw.png\"><figcaption>We can see the data in Success by running the steps we created one by one. To do this, we can see the data when we right-click and click on \u201clist\u00a0queue\u201d.</figcaption></figure><p>In SplitJson, we write $ into JsonPath Expression. Afterwards, we put the QueryRecord processor and create the Record Writer and Reader for JSON again and give them a different name. Output Grouping \u201cOne Line Per Object\u201d must be selected in the JsonRecordSetWriter settings for\u00a0Reader.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*HSNmAIIW9jPVDc6suspk8w.png\"><figcaption>We determine the details of our new data by clicking \u201cAdd Property\u201d in QueryRecord. We give the names of all variables and finally add the time when the data was processed with ${now():toNumber()} as timestamp. Adding timestamp is optional.</figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*9LyVXaRY-EFk7d1iWE1mSQ.png\"><figcaption>Our last action in Pipeline is to send the data to ElasticSearch. We use the PutElasticsearchHttp processor for this. For this, we need to provide the Elasticsearch URL.<br>The critical thing here is to give the correct path. Since we are running all services with Docker, this path will be local.<br>In the Index section in this field, we write the index name to which this data will be assigned in ElasticSearch.</figcaption></figure><p>Important Note:</p>\n<p><em>If a \u201cConnection Refused\u201d error is received in the localhost:9200 or 127.0.0.1:9200 urls, go to Terminal and type \u201c</em><strong><em>hostname\u201d</em></strong><em> and the result should be written instead of localhost or 127.0.0.1.</em></p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*lCibLwod5RHZB28gbKahLA.png\"><figcaption>After these operations, we run each processor and finish the flow. We put a Funnel at the end of the stream. Let\u2019s start ElasticSearch and Kibana operations by going to the URL <a href=\"http://127.0.0.1:5601/\">http://127.0.0.1:5601</a>.</figcaption></figure><blockquote>Visualize Your Data with\u00a0Kibana</blockquote>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/502/1*Ms2UMuBit48c0EblUaxa5Q.png\"><figcaption>We see the index we sent with NiFi by entering Index Management in the Stock Management tab.</figcaption></figure><p>To visualize the data in this index, we enter Index Patterns under Kibana in the Stock Management tab and create a new\u00a0pattern.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*KNaFoDJf7mwpeIcbcE_Rgg.png\"><figcaption>Afterwards, we enter the Dashboard from the Analytics section and use the data as we wish. This is the part that is left to your imagination\u00a0:)</figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*C47iD25tL7fKEs21oK9K5Q.png\"></figure><blockquote>Sources:</blockquote>\n<p><a href=\"https://hub.docker.com/_/kibana\">https://hub.docker.com/_/kibana</a></p>\n<p><a href=\"https://www.linkedin.com/pulse/installing-apache-nifi-ubuntu-2004-lts-aws-dhruv-sahu/\">https://www.linkedin.com/pulse/installing-apache-nifi-ubuntu-2004-lts-aws-dhruv-sahu/</a></p>\n<p><a href=\"https://www.youtube.com/watch?v=U8dNNYeiD9o\">https://www.youtube.com/watch?v=U8dNNYeiD9o</a></p>\n<p><a href=\"https://www.youtube.com/watch?v=QXjcLenjz0k\">https://www.youtube.com/watch?v=QXjcLenjz0k</a></p>\n<p><a href=\"https://hub.docker.com/_/elasticsearch\">https://hub.docker.com/_/elasticsearch</a></p>\n<p><a href=\"https://hub.docker.com/r/apache/nifi\">https://hub.docker.com/r/apache/nifi</a></p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=ba09563a5319\" width=\"1\" height=\"1\" alt=\"\">\n","enclosure":{},"categories":[]},{"title":"Get rid of typing \u201cpd.read_csv\u201d, store your data on database!","pubDate":"2024-01-14 17:36:11","link":"https://omeryasirkucuk.medium.com/get-rid-of-typing-pd-read-csv-store-your-data-on-database-1944b3014a0e?source=rss-6e7a0ab6f65d------2","guid":"https://medium.com/p/1944b3014a0e","author":"\u00d6mer Yasir K\u00fc\u00e7\u00fck","thumbnail":"","description":"\n<p>Do you have millions of lines of data that you need to use in your\u00a0work?</p>\n<p>Instead of using it on your computer, how about uploading it to a database and accessing it from anywhere?</p>\n<p>Ok then let\u2019s get started. GitHub is where we store our codes and files in joint projects. But we cannot add large files\u00a0here.</p>\n<p>We know that in the professional world, our data is in databases. Let\u2019s apply this to our own\u00a0data.</p>\n<p>You can use any data to test this work. I\u2019m taking a sample dataset from Kaggle. There are approximately 6.3M rows of\u00a0data.</p>\n<p>If I need to collaborate with my friends with this data and want to do this from a local IDE rather than a public compiler, using a database solution so that everyone can pull the data together will make my job\u00a0easier.</p>\n<p>I prefer AWS for database solution. Thanks to the free tier on AWS, you can use 750 hours of monthly usage and 20 GB of space for\u00a0free.</p>\n<p>We use the RDS service for AWS database solutions. Here you can find many database services including MySQL, Oracle, Postgre. I prefer\u00a0Postgre.</p>\n<p>The most important point here is to choose the Security Group correctly so that you can use the database you created from any point you want. We create the Security Group under the EC2\u00a0service.</p>\n<h3><strong>Connection to postgre in AWS with\u00a0Python</strong></h3>\n<pre>import psycopg2<br><br>conn = psycopg2.connect(<br>    host=\"your_host_info\",<br>    port=your_port_info,<br>    user=\"your_user_name\",<br>    password=\"your_password\"<br>)<br><br>conn.autocommit=True<br>cursor = conn.cursor()</pre>\n<p>If you click your database you want to connect with python, you can show host information RDS &gt;&gt; Database&gt;&gt;your database</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*8xYb6KPBInsi5LVQQUEA7w.png\"><figcaption>your host information</figcaption></figure><p>After making our connection, we can create a table. The script to create a new table for the data I selected is as\u00a0follows:</p>\n<pre>create_table_paysim1 = \"\"\"<br>CREATE TABLE IF NOT EXISTS paysim1 (<br>    step INT,<br>    type VARCHAR(50),<br>    amount DECIMAL(18, 2),<br>    nameOrig VARCHAR(50),<br>    oldbalanceOrg DECIMAL(18, 2),<br>    newbalanceOrig DECIMAL(18, 2),<br>    nameDest VARCHAR(50),<br>    oldbalanceDest DECIMAL(18, 2),<br>    newbalanceDest DECIMAL(18, 2),<br>    isFraud INT,<br>    isFlaggedFraud INT<br>);<br><br>\"\"\"<br><br>cursor.execute(create_table_paysim1)</pre>\n<h3>Inserting Phase</h3>\n<p>The critical part comes from now on; If you do not have a big data solution and want to insert a large local data at once, it is not correct to write \u201cINSERT INTO\u201d one by one. Or, it does not make sense to read the relevant\u00a0.csv file line by line with Python and insert it because you can only insert a limited amount of data per unit\u00a0time.</p>\n<p>You can \u201ccopy\u201d any data you have to psql after making your post connection from bash. The example script is as\u00a0follows:</p>\n<h4>connect to postgres on\u00a0bash</h4>\n<pre>psql \\<br>   --host=&lt;DB instance endpoint&gt; \\<br>   --port=&lt;port&gt; \\<br>   --username=&lt;master username&gt; \\<br>   --password \\<br>   --dbname=&lt;database name&gt;</pre>\n<p><strong>copy data from local to\u00a0postgres</strong></p>\n<pre>copy public.paysim1 (step, type, amount, nameorig, oldbalanceorg, newbalanceorig, namedest, oldbalancedest, newbalancedest, isfraud, isflaggedfraud) FROM '/path/paysim1.csv' DELIMITER ',' CSV HEADER QUOTE '\\\"' ESCAPE '''';\"\"</pre>\n<p>If you have difficulty doing this or connecting, there are interfaces that can do this for\u00a0you!</p>\n<p>Some free services where you can easily write your Postgresql queries and manage your tables: pgAdmin,\u00a0DBeaver</p>\n<p>Let me show you how to import\u00a0.csv data in both of\u00a0them.</p>\n<h4>DBeaver</h4>\n<p>First of all, you connect your database:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/820/1*r8DbDdhrplQSx4Nrc6o-5A.png\"></figure><p>After that, find your table on Database Navigator and click\u00a0right:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/769/1*p7J8ujSrL6QkeGlbmQh1kg.png\"></figure><h4>pgAdmin</h4>\n<p>Actually, the process is same as DBeaver. Create a connection, right click and import data. But for this, I want to show how it can be imported.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*1Yek8Dl3w9QPiYBnEgFXVQ.png\"></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*0NEe50e0CJjoe0D86O6cRA.png\"></figure><p>You should spesify header, delimiter, path, escape info according to your\u00a0data.</p>\n<p>But with this way, you can import your data include over 6 M row data under 6 minutes (it is about your connection)</p>\n<pre>cursor.execute(\"\"\" SELECT * FROM paysim1;)</pre>\n<p>Yes, now all you have to do is select your data and use it. In this way, you can manage your database the way you\u00a0want!</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=1944b3014a0e\" width=\"1\" height=\"1\" alt=\"\">\n","content":"\n<p>Do you have millions of lines of data that you need to use in your\u00a0work?</p>\n<p>Instead of using it on your computer, how about uploading it to a database and accessing it from anywhere?</p>\n<p>Ok then let\u2019s get started. GitHub is where we store our codes and files in joint projects. But we cannot add large files\u00a0here.</p>\n<p>We know that in the professional world, our data is in databases. Let\u2019s apply this to our own\u00a0data.</p>\n<p>You can use any data to test this work. I\u2019m taking a sample dataset from Kaggle. There are approximately 6.3M rows of\u00a0data.</p>\n<p>If I need to collaborate with my friends with this data and want to do this from a local IDE rather than a public compiler, using a database solution so that everyone can pull the data together will make my job\u00a0easier.</p>\n<p>I prefer AWS for database solution. Thanks to the free tier on AWS, you can use 750 hours of monthly usage and 20 GB of space for\u00a0free.</p>\n<p>We use the RDS service for AWS database solutions. Here you can find many database services including MySQL, Oracle, Postgre. I prefer\u00a0Postgre.</p>\n<p>The most important point here is to choose the Security Group correctly so that you can use the database you created from any point you want. We create the Security Group under the EC2\u00a0service.</p>\n<h3><strong>Connection to postgre in AWS with\u00a0Python</strong></h3>\n<pre>import psycopg2<br><br>conn = psycopg2.connect(<br>    host=\"your_host_info\",<br>    port=your_port_info,<br>    user=\"your_user_name\",<br>    password=\"your_password\"<br>)<br><br>conn.autocommit=True<br>cursor = conn.cursor()</pre>\n<p>If you click your database you want to connect with python, you can show host information RDS &gt;&gt; Database&gt;&gt;your database</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*8xYb6KPBInsi5LVQQUEA7w.png\"><figcaption>your host information</figcaption></figure><p>After making our connection, we can create a table. The script to create a new table for the data I selected is as\u00a0follows:</p>\n<pre>create_table_paysim1 = \"\"\"<br>CREATE TABLE IF NOT EXISTS paysim1 (<br>    step INT,<br>    type VARCHAR(50),<br>    amount DECIMAL(18, 2),<br>    nameOrig VARCHAR(50),<br>    oldbalanceOrg DECIMAL(18, 2),<br>    newbalanceOrig DECIMAL(18, 2),<br>    nameDest VARCHAR(50),<br>    oldbalanceDest DECIMAL(18, 2),<br>    newbalanceDest DECIMAL(18, 2),<br>    isFraud INT,<br>    isFlaggedFraud INT<br>);<br><br>\"\"\"<br><br>cursor.execute(create_table_paysim1)</pre>\n<h3>Inserting Phase</h3>\n<p>The critical part comes from now on; If you do not have a big data solution and want to insert a large local data at once, it is not correct to write \u201cINSERT INTO\u201d one by one. Or, it does not make sense to read the relevant\u00a0.csv file line by line with Python and insert it because you can only insert a limited amount of data per unit\u00a0time.</p>\n<p>You can \u201ccopy\u201d any data you have to psql after making your post connection from bash. The example script is as\u00a0follows:</p>\n<h4>connect to postgres on\u00a0bash</h4>\n<pre>psql \\<br>   --host=&lt;DB instance endpoint&gt; \\<br>   --port=&lt;port&gt; \\<br>   --username=&lt;master username&gt; \\<br>   --password \\<br>   --dbname=&lt;database name&gt;</pre>\n<p><strong>copy data from local to\u00a0postgres</strong></p>\n<pre>copy public.paysim1 (step, type, amount, nameorig, oldbalanceorg, newbalanceorig, namedest, oldbalancedest, newbalancedest, isfraud, isflaggedfraud) FROM '/path/paysim1.csv' DELIMITER ',' CSV HEADER QUOTE '\\\"' ESCAPE '''';\"\"</pre>\n<p>If you have difficulty doing this or connecting, there are interfaces that can do this for\u00a0you!</p>\n<p>Some free services where you can easily write your Postgresql queries and manage your tables: pgAdmin,\u00a0DBeaver</p>\n<p>Let me show you how to import\u00a0.csv data in both of\u00a0them.</p>\n<h4>DBeaver</h4>\n<p>First of all, you connect your database:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/820/1*r8DbDdhrplQSx4Nrc6o-5A.png\"></figure><p>After that, find your table on Database Navigator and click\u00a0right:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/769/1*p7J8ujSrL6QkeGlbmQh1kg.png\"></figure><h4>pgAdmin</h4>\n<p>Actually, the process is same as DBeaver. Create a connection, right click and import data. But for this, I want to show how it can be imported.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*1Yek8Dl3w9QPiYBnEgFXVQ.png\"></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*0NEe50e0CJjoe0D86O6cRA.png\"></figure><p>You should spesify header, delimiter, path, escape info according to your\u00a0data.</p>\n<p>But with this way, you can import your data include over 6 M row data under 6 minutes (it is about your connection)</p>\n<pre>cursor.execute(\"\"\" SELECT * FROM paysim1;)</pre>\n<p>Yes, now all you have to do is select your data and use it. In this way, you can manage your database the way you\u00a0want!</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=1944b3014a0e\" width=\"1\" height=\"1\" alt=\"\">\n","enclosure":{},"categories":[]},{"title":"Building a Data Streaming Pipeline: Raspberry Pi 5","pubDate":"2023-12-11 19:35:39","link":"https://omeryasirkucuk.medium.com/building-a-data-streaming-pipeline-raspberry-pi-5-17c6d9e11fc0?source=rss-6e7a0ab6f65d------2","guid":"https://medium.com/p/17c6d9e11fc0","author":"\u00d6mer Yasir K\u00fc\u00e7\u00fck","thumbnail":"","description":"\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*dHyAKYhD7Dma-OUPx4NiTA.png\"></figure><p>Raspberry Pi 5 was introduced in recent months. I bought one to try out its many features such as faster processor power, embedded wifi and bluetooth.</p>\n<p>As with previous Raspberry versions, it is possible to develop many projects with this new device. It gives you the opportunity to experience the most popular topics of recent years such as Industry 4.0 and\u00a0IOT.</p>\n<p>In fact, with this project, I wanted to test how sensor data received using Raspberry can be processed at a simple user level and with big data environment opportunities.</p>\n<p>I used simple sensors to get sensor data on Raspberry. You can do this project, which I made by installing a PIR sensor and LED, by combining dozens of different sensors such as sound sensor, distance sensor, temperature meter, gas meter, weight meter, etc. according to your own project. It is possible to create not only sensors, but also thousands of project ideas where you can use the power of Raspberry.</p>\n<p>However, the main goal here is not to create a complicated embedded system, but to obtain a data pool that arrives regularly and in short periods of time, which we can define as <strong>big\u00a0data</strong>.</p>\n<p>You can find the project details and codes in the\u00a0below.</p>\n<h4>Get and Save Data from sensors connected to Raspberry</h4>\n<p>I logged the data I received every 3 seconds to ensure that it was compatible with the PIR sensor I had. I could transmit this data through an API, but Firebase is a very useful platform where I can take quick action. It can be easily controlled with Python using the firebase_admin library. If you wish, you can also use Firebase\u2019s APIs.</p>\n<pre><br>from gpiozero import MotionSensor, LED<br>import time<br>from datetime import datetime<br>pir = MotionSensor(27)<br>led_pin = 17<br>led = LED(led_pin)<br><br><br>import firebase_admin<br>from firebase_admin import credentials, firestore<br><br><br>cred = credentials.Certificate(YOUR_JSON_CERTIFICATE_FILE_PATH)<br>firebase_admin.initialize_app(cred)<br><br>db = firestore.client()<br><br>collection_path = \"logs\"<br><br><br>try:<br>    time.sleep(2)<br>    while True:<br>        timestamp_name = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")<br>        if pir.motion_detected:<br>            led.on()<br>            timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")<br>            data_to_send = {<br>                \"room\": \"oyk_sahsi\",<br>                \"led_off_on\": \"True\",<br>                \"detection\": \"True\",<br>                \"timestamp\": timestamp<br>            }<br>        else:<br>            led.off()<br>            timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")<br>            data_to_send = {<br>                \"room\": \"oyk_sahsi\",<br>                \"led_off_on\": \"False\",<br>                \"detection\": \"False\",<br>                \"timestamp\": timestamp<br>            }<br>        document_name = f\"log_{timestamp_name}\"<br>        db.collection(collection_path).document(document_name).set(data_to_send)<br>        time.sleep(3)<br><br>except Exception as e:<br>    print(f\"Hata: {e}\")<br>    print(\"\u0130\u015flem iptal\")</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*1QajyRdRJXdAKnbDZ9qhow.png\"></figure><p>We used Firebase to save instant data, but Firebase provides a free version up to a certain capacity. For this reason, I chose to move this data to Hadoop HDFS on my local\u00a0site.</p>\n<h4>Transfer Data to\u00a0HDFS</h4>\n<p>Of course, it doesn\u2019t make sense to save all the data in Firebase every time. For this reason, if the data was previously saved as JSON in the HDFS environment, I applied the <strong>fetched: True</strong> tag to that data on FireBase. I also regularly kept data locally for some\u00a0tests.</p>\n<pre>import json<br>import firebase_admin<br>from firebase_admin import credentials, firestore<br>from hdfs import InsecureClient<br><br>cred = credentials.Certificate(<br>    \"YOUR_JSON_CERTIFICATE_FILE_PATH\"<br>)<br><br><br>if not firebase_admin._apps:<br>    firebase_admin.initialize_app(cred)<br><br>db = firestore.client()<br>collection_path = \"logs\"<br><br><br>def export_new_data():<br>    hdfs_client = InsecureClient('http://localhost:xxxx', user='username')<br><br>     try:<br>        with hdfs_client.read('hdfs_json_path', encoding='utf-8') as reader:<br>            all_data = json.load(reader)<br>    except (FileNotFoundError, json.decoder.JSONDecodeError):<br>        all_data = []<br><br>    sensor_ref = db.collection(collection_path)<br>    docs = sensor_ref.stream()<br><br>    new_data = []<br>    for doc in docs:<br>        data = doc.to_dict()<br>        timestamp = data.get(\"timestamp\")<br><br>        if not any(existing_data.get(\"timestamp\") == timestamp for existing_data in all_data):<br>            new_data.append(data)<br><br>                       doc.reference.update({\"fetched\": \"True\"})<br>            data[\"fetched\"] = \"True\"<br><br>    all_data.extend(new_data)<br><br>    json_data = json.dumps(all_data)<br>    with hdfs_client.write('hdfs_json_path', encoding='utf-8', overwrite=True) as writer:<br>        writer.write(json_data)<br><br><br>    hdfs_client.download('hdfs_json_path', 'local_json_path',<br>                         overwrite=True)<br><br><br><br>export_new_data()</pre>\n<h4>Peeping Data getting from sensors with Discord\u00a0Bot</h4>\n<p>Now our data flows to both Firestore and HDFS up to date. However, when we wanted to use this data, I thought it would be better to stream the sensor data that arrives in 3 seconds with\u00a0Kafka.</p>\n<p>This type of data is not only for storing but also for using. In this application, I wanted to send the data of the rooms where motion was detected to myself via\u00a0Discord.</p>\n<p>Managing Discord with Python is very simple, you can do these works with Discord for many purposes without the need for a different mobile\u00a0app.</p>\n<p>Kafka Producer:</p>\n<pre>import time<br>from kafka import KafkaProducer<br>import json<br>import os<br><br>bootstrap_servers = 'your_bootstrap_server'<br>kafka_topic = 'raspberrypilogs'<br>file_path = 'hdfs_json_path'<br><br>last_offset = 0<br><br>def read_and_send_to_kafka():<br>    global last_offset<br><br>    file_length = os.path.getsize(file_path)<br><br>    if last_offset &gt;= file_length:<br>        print(\"Reached end of json, waiting for new data from hdfs\")<br>        time.sleep(5)<br>        return<br><br>    with open(file_path, 'r') as hdfs_file:<br>        hdfs_file.seek(last_offset)<br><br>        producer = KafkaProducer(bootstrap_servers=bootstrap_servers,<br>                                 value_serializer=lambda v: json.dumps(v).encode('utf-8'))<br><br>        remaining_content = hdfs_file.read()<br><br>        for line in remaining_content.split('\\n'):<br>            if not line:<br>                continue<br><br>            try:<br><br>                data = json.loads(line)<br>                for i in data:<br>                    print(i)<br>                    producer.send(kafka_topic, value=i)<br><br>            except json.JSONDecodeError as e:<br>                print(f\"Hata: JSONDecodeError - {e}\")<br><br>        producer.close()<br>        last_offset = hdfs_file.tell()<br><br><br>if __name__ == \"__main__\":<br>    while True:<br>        read_and_send_to_kafka()<br>        time.sleep(5)</pre>\n<p>Kafka Consumer and Send Data to\u00a0Discord</p>\n<pre>import json<br>import discord<br>from kafka import KafkaConsumer<br>import asyncio<br><br>asyncio.set_event_loop(asyncio.new_event_loop())<br><br>bootstrap_servers = 'your_bootstrap_server'<br>topic_name = 'raspberrypilogs'<br>discord_bot_token = 'your_bot_token'<br><br>client = discord.Client()<br><br>@client.event<br>async def on_ready():<br>    print(f'We have logged in as {client.user}')<br><br>    consumer = KafkaConsumer(topic_name,<br>                             bootstrap_servers=bootstrap_servers,<br>                             value_deserializer=lambda v: json.loads(v.decode('utf-8')))<br><br>    try:<br>        for message in consumer:<br>            received_data = message.value<br><br>            if received_data[\"detection\"] == 'True':<br>                print(f\"Detection is True! Data: {received_data}\")<br><br><br>                channel_id = 'your_channel_id'<br>                channel = client.get_channel(int(channel_id))<br><br>                if channel:<br>                    room_name = received_data[\"room\"]<br>                    timestamp = received_data[\"timestamp\"]<br>                    message_content = f\"Room where motion was detected: {room_name}, Motion time: {timestamp}\"<br>                    await channel.send(message_content)<br>                else:<br>                    print(f\"Invalid channel ID: {channel_id}\")<br><br>    except KeyboardInterrupt:<br>        pass<br><br>    finally:<br>        consumer.close()<br><br>client.run(discord_bot_token)</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*-aYJOUMeJKXfo_ymHENovg.png\"></figure><p>Thus, I shared my experiences with you. I know that it is possible to do this project end-to-end with different technologies and different flows. But I aim to improve myself by doing projects in this\u00a0way.</p>\n<p>Note: <strong>Airflow</strong> seems to be the most reasonable option for all scripts to run at a certain interval. It will be implemented in this\u00a0project.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=17c6d9e11fc0\" width=\"1\" height=\"1\" alt=\"\">\n","content":"\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*dHyAKYhD7Dma-OUPx4NiTA.png\"></figure><p>Raspberry Pi 5 was introduced in recent months. I bought one to try out its many features such as faster processor power, embedded wifi and bluetooth.</p>\n<p>As with previous Raspberry versions, it is possible to develop many projects with this new device. It gives you the opportunity to experience the most popular topics of recent years such as Industry 4.0 and\u00a0IOT.</p>\n<p>In fact, with this project, I wanted to test how sensor data received using Raspberry can be processed at a simple user level and with big data environment opportunities.</p>\n<p>I used simple sensors to get sensor data on Raspberry. You can do this project, which I made by installing a PIR sensor and LED, by combining dozens of different sensors such as sound sensor, distance sensor, temperature meter, gas meter, weight meter, etc. according to your own project. It is possible to create not only sensors, but also thousands of project ideas where you can use the power of Raspberry.</p>\n<p>However, the main goal here is not to create a complicated embedded system, but to obtain a data pool that arrives regularly and in short periods of time, which we can define as <strong>big\u00a0data</strong>.</p>\n<p>You can find the project details and codes in the\u00a0below.</p>\n<h4>Get and Save Data from sensors connected to Raspberry</h4>\n<p>I logged the data I received every 3 seconds to ensure that it was compatible with the PIR sensor I had. I could transmit this data through an API, but Firebase is a very useful platform where I can take quick action. It can be easily controlled with Python using the firebase_admin library. If you wish, you can also use Firebase\u2019s APIs.</p>\n<pre><br>from gpiozero import MotionSensor, LED<br>import time<br>from datetime import datetime<br>pir = MotionSensor(27)<br>led_pin = 17<br>led = LED(led_pin)<br><br><br>import firebase_admin<br>from firebase_admin import credentials, firestore<br><br><br>cred = credentials.Certificate(YOUR_JSON_CERTIFICATE_FILE_PATH)<br>firebase_admin.initialize_app(cred)<br><br>db = firestore.client()<br><br>collection_path = \"logs\"<br><br><br>try:<br>    time.sleep(2)<br>    while True:<br>        timestamp_name = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")<br>        if pir.motion_detected:<br>            led.on()<br>            timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")<br>            data_to_send = {<br>                \"room\": \"oyk_sahsi\",<br>                \"led_off_on\": \"True\",<br>                \"detection\": \"True\",<br>                \"timestamp\": timestamp<br>            }<br>        else:<br>            led.off()<br>            timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")<br>            data_to_send = {<br>                \"room\": \"oyk_sahsi\",<br>                \"led_off_on\": \"False\",<br>                \"detection\": \"False\",<br>                \"timestamp\": timestamp<br>            }<br>        document_name = f\"log_{timestamp_name}\"<br>        db.collection(collection_path).document(document_name).set(data_to_send)<br>        time.sleep(3)<br><br>except Exception as e:<br>    print(f\"Hata: {e}\")<br>    print(\"\u0130\u015flem iptal\")</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*1QajyRdRJXdAKnbDZ9qhow.png\"></figure><p>We used Firebase to save instant data, but Firebase provides a free version up to a certain capacity. For this reason, I chose to move this data to Hadoop HDFS on my local\u00a0site.</p>\n<h4>Transfer Data to\u00a0HDFS</h4>\n<p>Of course, it doesn\u2019t make sense to save all the data in Firebase every time. For this reason, if the data was previously saved as JSON in the HDFS environment, I applied the <strong>fetched: True</strong> tag to that data on FireBase. I also regularly kept data locally for some\u00a0tests.</p>\n<pre>import json<br>import firebase_admin<br>from firebase_admin import credentials, firestore<br>from hdfs import InsecureClient<br><br>cred = credentials.Certificate(<br>    \"YOUR_JSON_CERTIFICATE_FILE_PATH\"<br>)<br><br><br>if not firebase_admin._apps:<br>    firebase_admin.initialize_app(cred)<br><br>db = firestore.client()<br>collection_path = \"logs\"<br><br><br>def export_new_data():<br>    hdfs_client = InsecureClient('http://localhost:xxxx', user='username')<br><br>     try:<br>        with hdfs_client.read('hdfs_json_path', encoding='utf-8') as reader:<br>            all_data = json.load(reader)<br>    except (FileNotFoundError, json.decoder.JSONDecodeError):<br>        all_data = []<br><br>    sensor_ref = db.collection(collection_path)<br>    docs = sensor_ref.stream()<br><br>    new_data = []<br>    for doc in docs:<br>        data = doc.to_dict()<br>        timestamp = data.get(\"timestamp\")<br><br>        if not any(existing_data.get(\"timestamp\") == timestamp for existing_data in all_data):<br>            new_data.append(data)<br><br>                       doc.reference.update({\"fetched\": \"True\"})<br>            data[\"fetched\"] = \"True\"<br><br>    all_data.extend(new_data)<br><br>    json_data = json.dumps(all_data)<br>    with hdfs_client.write('hdfs_json_path', encoding='utf-8', overwrite=True) as writer:<br>        writer.write(json_data)<br><br><br>    hdfs_client.download('hdfs_json_path', 'local_json_path',<br>                         overwrite=True)<br><br><br><br>export_new_data()</pre>\n<h4>Peeping Data getting from sensors with Discord\u00a0Bot</h4>\n<p>Now our data flows to both Firestore and HDFS up to date. However, when we wanted to use this data, I thought it would be better to stream the sensor data that arrives in 3 seconds with\u00a0Kafka.</p>\n<p>This type of data is not only for storing but also for using. In this application, I wanted to send the data of the rooms where motion was detected to myself via\u00a0Discord.</p>\n<p>Managing Discord with Python is very simple, you can do these works with Discord for many purposes without the need for a different mobile\u00a0app.</p>\n<p>Kafka Producer:</p>\n<pre>import time<br>from kafka import KafkaProducer<br>import json<br>import os<br><br>bootstrap_servers = 'your_bootstrap_server'<br>kafka_topic = 'raspberrypilogs'<br>file_path = 'hdfs_json_path'<br><br>last_offset = 0<br><br>def read_and_send_to_kafka():<br>    global last_offset<br><br>    file_length = os.path.getsize(file_path)<br><br>    if last_offset &gt;= file_length:<br>        print(\"Reached end of json, waiting for new data from hdfs\")<br>        time.sleep(5)<br>        return<br><br>    with open(file_path, 'r') as hdfs_file:<br>        hdfs_file.seek(last_offset)<br><br>        producer = KafkaProducer(bootstrap_servers=bootstrap_servers,<br>                                 value_serializer=lambda v: json.dumps(v).encode('utf-8'))<br><br>        remaining_content = hdfs_file.read()<br><br>        for line in remaining_content.split('\\n'):<br>            if not line:<br>                continue<br><br>            try:<br><br>                data = json.loads(line)<br>                for i in data:<br>                    print(i)<br>                    producer.send(kafka_topic, value=i)<br><br>            except json.JSONDecodeError as e:<br>                print(f\"Hata: JSONDecodeError - {e}\")<br><br>        producer.close()<br>        last_offset = hdfs_file.tell()<br><br><br>if __name__ == \"__main__\":<br>    while True:<br>        read_and_send_to_kafka()<br>        time.sleep(5)</pre>\n<p>Kafka Consumer and Send Data to\u00a0Discord</p>\n<pre>import json<br>import discord<br>from kafka import KafkaConsumer<br>import asyncio<br><br>asyncio.set_event_loop(asyncio.new_event_loop())<br><br>bootstrap_servers = 'your_bootstrap_server'<br>topic_name = 'raspberrypilogs'<br>discord_bot_token = 'your_bot_token'<br><br>client = discord.Client()<br><br>@client.event<br>async def on_ready():<br>    print(f'We have logged in as {client.user}')<br><br>    consumer = KafkaConsumer(topic_name,<br>                             bootstrap_servers=bootstrap_servers,<br>                             value_deserializer=lambda v: json.loads(v.decode('utf-8')))<br><br>    try:<br>        for message in consumer:<br>            received_data = message.value<br><br>            if received_data[\"detection\"] == 'True':<br>                print(f\"Detection is True! Data: {received_data}\")<br><br><br>                channel_id = 'your_channel_id'<br>                channel = client.get_channel(int(channel_id))<br><br>                if channel:<br>                    room_name = received_data[\"room\"]<br>                    timestamp = received_data[\"timestamp\"]<br>                    message_content = f\"Room where motion was detected: {room_name}, Motion time: {timestamp}\"<br>                    await channel.send(message_content)<br>                else:<br>                    print(f\"Invalid channel ID: {channel_id}\")<br><br>    except KeyboardInterrupt:<br>        pass<br><br>    finally:<br>        consumer.close()<br><br>client.run(discord_bot_token)</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*-aYJOUMeJKXfo_ymHENovg.png\"></figure><p>Thus, I shared my experiences with you. I know that it is possible to do this project end-to-end with different technologies and different flows. But I aim to improve myself by doing projects in this\u00a0way.</p>\n<p>Note: <strong>Airflow</strong> seems to be the most reasonable option for all scripts to run at a certain interval. It will be implemented in this\u00a0project.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=17c6d9e11fc0\" width=\"1\" height=\"1\" alt=\"\">\n","enclosure":{},"categories":["hdfs","airflow","kafka","raspberry-pi-5","streaming"]},{"title":"Content Based Recommendation by using Actor/Actress Names on Netflix Movies Data","pubDate":"2021-12-28 18:18:24","link":"https://omeryasirkucuk.medium.com/content-based-recommendation-by-using-actor-actress-names-on-netflix-movies-data-bf434700d759?source=rss-6e7a0ab6f65d------2","guid":"https://medium.com/p/bf434700d759","author":"\u00d6mer Yasir K\u00fc\u00e7\u00fck","thumbnail":"","description":"\n<p>Purposes: I aim to find closest movies or tv series based on casts similarity. I use TF-IDF and Cosine Similarity Matris for finding top 10 similar\u00a0movies.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*4uXzSZRdKZU2U6XB\"><figcaption>Photo by <a href=\"https://unsplash.com/@tamasp?utm_source=medium&amp;utm_medium=referral\">Tamas Pap</a> on\u00a0<a href=\"https://unsplash.com/?utm_source=medium&amp;utm_medium=referral\">Unsplash</a></figcaption></figure><p>Let\u2019s start.</p>\n<blockquote>First of all, we must import libraries.</blockquote>\n<pre>import pandas as pd<br>from sklearn.feature_extraction.text import TfidfVectorizer<br>from sklearn.metrics.pairwise import cosine_similarity</pre>\n<blockquote>And then, importing the datas from\u00a0.csv\u00a0file.</blockquote>\n<pre>df = pd.read_csv(\"../input/netflix-shows/netflix_titles.csv\")<br>df.shape  #(8807, 12)<br>df.head(50)</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*Dxh0JU6JM2_0grSBfhkr2g.png\"></figure><blockquote>Are there NaN values in Cast columns? If there are, filling the space \u201c \u201c instead\u00a0NaN</blockquote>\n<pre>df[\"cast\"].isna().sum()<br>df[\"cast\"] = df[\"cast\"].fillna(\" \")<br>df[\"cast\"].head(50)</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/614/1*C3c2HlEvXIBqPsTzBhUX7g.png\"></figure><blockquote><em>Setting up Tf-Idf\u00a0matrix</em></blockquote>\n<pre>tfidf = TfidfVectorizer(stop_words=\"english\")<br>tf_idf_matrix = tfidf.fit_transform(df[\"cast\"])<br>type(tf_idf_matrix) <em>#scipy.sparse.csr.csr_matrix</em><br>df[\"cast\"].shape <em>#(8807,)</em><br>tf_idf_matrix.shape <em>#(8807, 31799)</em></pre>\n<blockquote>Making recommendation based on similarities</blockquote>\n<pre><em>#Picking film names</em><br>indices = pd.Series(df.index, index=df[\"title\"])</pre>\n<pre><em>#Deleting duplicate values in indices</em><br>indices = indices[~indices.index.duplicated(keep=\"last\")]</pre>\n<pre><em>#Choosing a film in the df for finding its similar.</em><br>df[df[\"title\"].str.contains(\"Vizon\",na=False)] <em>#Checking Inception is on the list or not.</em><br>movie_index = indices[\"Vizontele\"]</pre>\n<pre><em>#Converting pandas dataframe similarities between Inception and other films based on casts.</em><br>similarity_scores = pd.DataFrame(cosine_sim[movie_index], columns=[\"Score\"])</pre>\n<pre><em>#Taking most similar films' index except itself (Vizontele)</em><br>movie_indices = similarity_scores.sort_values(\"Score\", ascending=False)[1:11].index</pre>\n<pre><em>#Showing the 10 movies that the casts are most similar</em><br>df[[\"title\",\"cast\"]].iloc[movie_indices]</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/812/1*JKC9ijf7-O-zSZCqBsjsXg.png\"><figcaption>10 Movies most similar to Vizontele</figcaption></figure><p>We can make recommendations using comments, titles, player names and similar variables in Content Based Filtering. In this analysis, we used the movie Vizontele made in Turkey. \u201cVizontele Tuuba\u201d was the second most similar movie in our analysis based on the similarity of the actors\u2019\u00a0names.</p>\n<p>Thanks to <a href=\"https://www.kaggle.com/shivamb/netflix-shows\">Shivam Bansal</a> for the dataset. I would also like to thank my biggest supporters in my self-development, VBO team and Vahit Keskin. I have cited sources and codes from\u00a0them.</p>\n<p>VBO: <a href=\"https://www.veribilimiokulu.com/\">https://www.veribilimiokulu.com/</a></p>\n<p>Vahit Keskin: <a href=\"https://www.linkedin.com/in/vahitkeskin/\">https://www.linkedin.com/in/vahitkeskin/</a></p>\n<p>Also if you want, you can reach Kaggle\u00a0codes.</p>\n<p><a href=\"https://www.kaggle.com/omeryasirkucuk/content-based-recommendation-w-netflix-dataset\">Content Based Recommendation w/Netflix Dataset</a></p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=bf434700d759\" width=\"1\" height=\"1\" alt=\"\">\n","content":"\n<p>Purposes: I aim to find closest movies or tv series based on casts similarity. I use TF-IDF and Cosine Similarity Matris for finding top 10 similar\u00a0movies.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*4uXzSZRdKZU2U6XB\"><figcaption>Photo by <a href=\"https://unsplash.com/@tamasp?utm_source=medium&amp;utm_medium=referral\">Tamas Pap</a> on\u00a0<a href=\"https://unsplash.com/?utm_source=medium&amp;utm_medium=referral\">Unsplash</a></figcaption></figure><p>Let\u2019s start.</p>\n<blockquote>First of all, we must import libraries.</blockquote>\n<pre>import pandas as pd<br>from sklearn.feature_extraction.text import TfidfVectorizer<br>from sklearn.metrics.pairwise import cosine_similarity</pre>\n<blockquote>And then, importing the datas from\u00a0.csv\u00a0file.</blockquote>\n<pre>df = pd.read_csv(\"../input/netflix-shows/netflix_titles.csv\")<br>df.shape  #(8807, 12)<br>df.head(50)</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*Dxh0JU6JM2_0grSBfhkr2g.png\"></figure><blockquote>Are there NaN values in Cast columns? If there are, filling the space \u201c \u201c instead\u00a0NaN</blockquote>\n<pre>df[\"cast\"].isna().sum()<br>df[\"cast\"] = df[\"cast\"].fillna(\" \")<br>df[\"cast\"].head(50)</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/614/1*C3c2HlEvXIBqPsTzBhUX7g.png\"></figure><blockquote><em>Setting up Tf-Idf\u00a0matrix</em></blockquote>\n<pre>tfidf = TfidfVectorizer(stop_words=\"english\")<br>tf_idf_matrix = tfidf.fit_transform(df[\"cast\"])<br>type(tf_idf_matrix) <em>#scipy.sparse.csr.csr_matrix</em><br>df[\"cast\"].shape <em>#(8807,)</em><br>tf_idf_matrix.shape <em>#(8807, 31799)</em></pre>\n<blockquote>Making recommendation based on similarities</blockquote>\n<pre><em>#Picking film names</em><br>indices = pd.Series(df.index, index=df[\"title\"])</pre>\n<pre><em>#Deleting duplicate values in indices</em><br>indices = indices[~indices.index.duplicated(keep=\"last\")]</pre>\n<pre><em>#Choosing a film in the df for finding its similar.</em><br>df[df[\"title\"].str.contains(\"Vizon\",na=False)] <em>#Checking Inception is on the list or not.</em><br>movie_index = indices[\"Vizontele\"]</pre>\n<pre><em>#Converting pandas dataframe similarities between Inception and other films based on casts.</em><br>similarity_scores = pd.DataFrame(cosine_sim[movie_index], columns=[\"Score\"])</pre>\n<pre><em>#Taking most similar films' index except itself (Vizontele)</em><br>movie_indices = similarity_scores.sort_values(\"Score\", ascending=False)[1:11].index</pre>\n<pre><em>#Showing the 10 movies that the casts are most similar</em><br>df[[\"title\",\"cast\"]].iloc[movie_indices]</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/812/1*JKC9ijf7-O-zSZCqBsjsXg.png\"><figcaption>10 Movies most similar to Vizontele</figcaption></figure><p>We can make recommendations using comments, titles, player names and similar variables in Content Based Filtering. In this analysis, we used the movie Vizontele made in Turkey. \u201cVizontele Tuuba\u201d was the second most similar movie in our analysis based on the similarity of the actors\u2019\u00a0names.</p>\n<p>Thanks to <a href=\"https://www.kaggle.com/shivamb/netflix-shows\">Shivam Bansal</a> for the dataset. I would also like to thank my biggest supporters in my self-development, VBO team and Vahit Keskin. I have cited sources and codes from\u00a0them.</p>\n<p>VBO: <a href=\"https://www.veribilimiokulu.com/\">https://www.veribilimiokulu.com/</a></p>\n<p>Vahit Keskin: <a href=\"https://www.linkedin.com/in/vahitkeskin/\">https://www.linkedin.com/in/vahitkeskin/</a></p>\n<p>Also if you want, you can reach Kaggle\u00a0codes.</p>\n<p><a href=\"https://www.kaggle.com/omeryasirkucuk/content-based-recommendation-w-netflix-dataset\">Content Based Recommendation w/Netflix Dataset</a></p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=bf434700d759\" width=\"1\" height=\"1\" alt=\"\">\n","enclosure":{},"categories":["content-based-filtering","netflix","similarity"]},{"title":"Exploratory Data Analysis (EDA) w/Market Sales Dataset","pubDate":"2021-12-12 22:13:16","link":"https://omeryasirkucuk.medium.com/exploratory-data-analysis-eda-w-market-sales-dataset-6f284269b195?source=rss-6e7a0ab6f65d------2","guid":"https://medium.com/p/6f284269b195","author":"\u00d6mer Yasir K\u00fc\u00e7\u00fck","thumbnail":"","description":"\n<p>In order to share my understanding of the concepts and techniques I know, I will take an example from the Market Sales dataset available on Kaggle and try to get as much insight as possible from the dataset using EDA. I got the Market Sales data set from the <a href=\"https://www.kaggle.com/omercolakoglu\">Omer Colakoglu</a>\u2019s data set that one of the leading SQL and database instructor in Turkey, shared publicly on Kaggle. I thank him for the data\u00a0set.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*7lkZW2Z25Xd-Sieu\"><figcaption>Photo by <a href=\"https://unsplash.com/@jessyoucan?utm_source=medium&amp;utm_medium=referral\">Jess Torre</a> on\u00a0<a href=\"https://unsplash.com/?utm_source=medium&amp;utm_medium=referral\">Unsplash</a></figcaption></figure><p>Here is a quick overview of the things that I am going to try in this\u00a0article:</p>\n<ul>\n<li>Data overview</li>\n<li>Analysis of categorical and numerical variables</li>\n<li>Correlation analysis</li>\n<li>Grouping and aggregation of datas for some\u00a0insights</li>\n<li>Visualization</li>\n</ul>\n<p>First of all, the libraries to be used were imported.</p>\n<pre>import pandas as pd #for using dataframes, importing datasets<br>import numpy as np #for mathematical operations<br>import seaborn as sns #for visualization<br>import matplotlib.pyplot as plt #for visualization<br>from openpyxl import Workbook #for importing excel workbook</pre>\n<p>After importing the libraries, we need to import the dataset. At this stage, since we will get the data set from Kaggle, we can do this using the Kaggle API if we wish, but since I downloaded this data set to my computer, I use Pandas\u2019 \u201cread_excel\u201d method.</p>\n<pre>df = pd.read_excel(\"dataset/MarketSales.xlsx\")</pre>\n<blockquote>Data Overview</blockquote>\n<p>Before starting the operations on the dataset, some Pandas methods such as head, tail and describe are used in order to understand the variables in the dataset correctly. Let\u2019s look at the data set by doing these in\u00a0order:</p>\n<pre>pd.set_option('display.max_columns', None)#In large datasets with many columns, it makes all columns appear in the console.</pre>\n<pre>#Shape<br>df.shape<br><br>#Types<br>df.dtypes<br><br>#Head<br>df.head(20)<br><br>#Tail<br>df.tail(20)<br><br>#Describe<br>df.describe().T<br><br>#Total Numbers of Nulls<br>df.isnull().sum()</pre>\n<p>With the methods used above, it can be seen that there are 611107 rows and 26 columns in the data set. In the data set, the number, prices and categories of the products purchased by the customers can be accessed, at the same time, it is possible to view who bought from which store and when, and their personal information.</p>\n<blockquote>Analysis of Categorical and Numerical Variables</blockquote>\n<p>I am trying to understand which is categorical and which is cardinal variable by observing the frequency of the data inside the variables.</p>\n<pre>[print(col, df[col].dtypes,df[col].nunique()) for col in df.columns]</pre>\n<p>Although the types of variables such as clientcode, clientname are object, I did not include them in my categorical data group because their frequencies are very high. I categorically take variables with a maximum frequency of\u00a0360.</p>\n<pre>cat_val = [col for col in df.columns if df[col].dtypes == \"O\" and df[col].nunique() &lt; 360]</pre>\n<p>I have included int and float values with a frequency less than 100 in my numeric but categorical variables.</p>\n<pre>num_but_cat = [col for col in df.columns if df[col].dtype in [int,float] and df[col].nunique() &lt; 100]</pre>\n<pre>cat_val = cat_val + num_but_cat</pre>\n<p>Numerical variables are \"AMOUNT\", \"PRICE\", \"LINENETTOTAL\", \"LINENET\".</p>\n<pre>num_cols = [col for col in [\"AMOUNT\", \"PRICE\", \"LINENETTOTAL\", \"LINENET\"]]</pre>\n<blockquote>Correlation Analysis</blockquote>\n<pre>corr = df[num_cols].corr()<br>corr</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/577/1*zL4ko0L-vm34uJ_qE6UuIw.png\"><figcaption><strong>Correlation Values (We can see there is high correlation between \u201cLINENET\u201d and \u201cLINENETTOTAL\u201d)</strong></figcaption></figure><blockquote><em>Visualization correlation among numerical variables</em></blockquote>\n<pre>sns.set(rc={'figure.figsize': (12, 12)})<br>sns.heatmap(corr, cmap=\"RdBu\")<br>plt.show()</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*hJKd8i-_ichctqe1b_uTwg.png\"></figure><p>or we can use another\u00a0method:</p>\n<pre>pip install klib<br>import klib<br>klib.corr_plot(corr)<br>plt.show()</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*d3M9bJN3DEJ05okQkUS9Jg.png\"></figure><p>For display relation between LINENET and LINENETTOTAL</p>\n<pre>plt.figure(figsize=(10,5))<br>sns.regplot(x=\"LINENET\", y=\"LINENETTOTAL\",data=df)<br>plt.xlim(0,400)<br>plt.ylim(0,400)<br>plt.show()</pre>\n<blockquote>Grouping and aggregation of datas for some\u00a0insights</blockquote>\n<pre><strong>#LINENETTOTAL's sum, mean, min and max values by GENDER</strong></pre>\n<pre>df[[\"LINENETTOTAL\",\"GENDER\"]].groupby(\"GENDER\").agg({\"LINENETTOTAL\":[\"sum\",\"mean\",\"min\", \"max\"]})</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/448/1*FwgT9uc_NJLZou5kPhAFjQ.png\"></figure><pre><strong>#AMOUNT totals by CATEGORY</strong><br><br>df[[\"CATEGORY_NAME1\",\"AMOUNT\"]].groupby([\"CATEGORY_NAME1\"]).agg({\"AMOUNT\":\"sum\"})</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/310/1*hYwjTeoibxj4-BF4LMibmA.png\"></figure><pre><strong>#Top 10 branches with the highest revenue</strong><br><br>df[[\"BRANCH\",\"LINENETTOTAL\"]].groupby([\"BRANCH\"]).agg({\"LINENETTOTAL\":\"sum\"}).sort_values(\"LINENETTOTAL\",ascending = False).head(10)</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/304/1*nHODTWWD29bC-aultIuSTw.png\"></figure><pre><strong>#Top 2 Region with the lowest revenue</strong><br><br>df[[\"REGION\",\"LINENETTOTAL\"]].groupby([\"REGION\"]).agg({\"LINENETTOTAL\":\"sum\"}).sort_values(\"LINENETTOTAL\",ascending = True).head(2)</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/320/1*odEVL2QCn_TnoslRlVQEvg.png\"></figure><pre><strong>#Sales totals and means according to <em>weekdays</em> and <em>weekends</em></strong><br><br>w_df = pd.DataFrame(pd.to_datetime(df['DATE_']))<br>w_df[\"WEEK_VARIABLE\"] = w_df[\"DATE_\"].dt.dayofweek<br>w_df[\"WEEKENDORNOT\"] = [\"WEEKEND\" if week_variable &gt; 4 else \"WEEKDAY\" for week_variable in w_df[\"WEEK_VARIABLE\"]]<br>w_df[\"LINENETTOTAL\"] = df[\"LINENETTOTAL\"]<br><br>w_df[[\"LINENETTOTAL\",\"WEEKENDORNOT\"]].groupby(\"WEEKENDORNOT\").agg({\"LINENETTOTAL\":[\"sum\",\"mean\"]})</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/367/1*CIJqZe4LtqVgQbo6EDLlDQ.png\"></figure><pre><strong>#Top 10 salesman with the highest total</strong><br><br>df[[\"SALESMAN\",\"LINENETTOTAL\"]].groupby([\"SALESMAN\"]).agg({\"LINENETTOTAL\":\"sum\"}).sort_values(\"LINENETTOTAL\",ascending = False).head(10)</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/316/1*vGH1zI5dvyrIAcGeluuP5w.png\"></figure><pre><strong>#Top 10 CLIENT with the highest total</strong><br><br>df[[\"CLIENTNAME\",\"CLIENTCODE\",\"LINENETTOTAL\"]].groupby([\"CLIENTNAME\",\"CLIENTCODE\"]).agg({\"LINENETTOTAL\":\"sum\"}).sort_values(\"LINENETTOTAL\",ascending = False).head(10)</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/480/1*Bwxl-xnpOFKROZ6vRS4_hg.png\"></figure><blockquote>Visualization</blockquote>\n<pre><strong>#Categorical Variables's Visulization</strong><br>for col in cat_val:<br>    sns.countplot(x=df[col], data=df)<br>    plt.show()</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/793/1*Nn8xwoHPfdYA2LgEcS0n1A.png\"></figure><pre><strong>#Distribution of linenettotal by gender</strong><br>sns.boxplot( x=df[\"GENDER\"], y=df[\"LINENETTOTAL\"] )<br>plt.ylim(0,50)<br>plt.show()</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*hzlgaEa8Ehv2LciYcAgL8g.png\"></figure><pre><strong>#View price ranges of categories</strong><br>ax = sns.barplot(x=\"CATEGORY_NAME1\", y=\"PRICE\", data=df)<br>ax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")<br>plt.show()</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*o1Qug7eQZgRcYUJc0vE9fA.png\"></figure><blockquote>Resources:</blockquote>\n<ul>\n<li><a href=\"https://www.python-graph-gallery.com/\">Python Graph Gallery | The Python Graph Gallery</a></li>\n<li><a href=\"https://python.plainenglish.io/lets-speed-up-exploratory-data-analysis-eda-with-the-klib-library-4cf93c81fd12\">Let\u2019s Speed Up Exploratory Data Analysis (EDA) with the Klib Library</a></li>\n<li><a href=\"https://analyticsindiamag.com/guide-to-mito-a-low-code-tool-for-exploratory-data-analysiseda/\">Guide To Mito - A Low Code Tool for Exploratory Data Analysis(EDA)</a></li>\n<li><a href=\"https://www.tutorialsrack.com/articles/324/how-to-find-the-current-day-is-weekday-or-weekends-in-python\">How to Find the Current Day is Weekday or Weekends in Python</a></li>\n<li><a href=\"https://www.kaggle.com/omercolakoglu/turkish-market-sales-dataset-with-9000items\">Turkish Market Sales Dataset With 9.000+Items</a></li>\n<li><a href=\"https://www.veribilimiokulu.com/\">VBO BLOG | Anasayfa</a></li>\n</ul>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=6f284269b195\" width=\"1\" height=\"1\" alt=\"\">\n","content":"\n<p>In order to share my understanding of the concepts and techniques I know, I will take an example from the Market Sales dataset available on Kaggle and try to get as much insight as possible from the dataset using EDA. I got the Market Sales data set from the <a href=\"https://www.kaggle.com/omercolakoglu\">Omer Colakoglu</a>\u2019s data set that one of the leading SQL and database instructor in Turkey, shared publicly on Kaggle. I thank him for the data\u00a0set.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*7lkZW2Z25Xd-Sieu\"><figcaption>Photo by <a href=\"https://unsplash.com/@jessyoucan?utm_source=medium&amp;utm_medium=referral\">Jess Torre</a> on\u00a0<a href=\"https://unsplash.com/?utm_source=medium&amp;utm_medium=referral\">Unsplash</a></figcaption></figure><p>Here is a quick overview of the things that I am going to try in this\u00a0article:</p>\n<ul>\n<li>Data overview</li>\n<li>Analysis of categorical and numerical variables</li>\n<li>Correlation analysis</li>\n<li>Grouping and aggregation of datas for some\u00a0insights</li>\n<li>Visualization</li>\n</ul>\n<p>First of all, the libraries to be used were imported.</p>\n<pre>import pandas as pd #for using dataframes, importing datasets<br>import numpy as np #for mathematical operations<br>import seaborn as sns #for visualization<br>import matplotlib.pyplot as plt #for visualization<br>from openpyxl import Workbook #for importing excel workbook</pre>\n<p>After importing the libraries, we need to import the dataset. At this stage, since we will get the data set from Kaggle, we can do this using the Kaggle API if we wish, but since I downloaded this data set to my computer, I use Pandas\u2019 \u201cread_excel\u201d method.</p>\n<pre>df = pd.read_excel(\"dataset/MarketSales.xlsx\")</pre>\n<blockquote>Data Overview</blockquote>\n<p>Before starting the operations on the dataset, some Pandas methods such as head, tail and describe are used in order to understand the variables in the dataset correctly. Let\u2019s look at the data set by doing these in\u00a0order:</p>\n<pre>pd.set_option('display.max_columns', None)#In large datasets with many columns, it makes all columns appear in the console.</pre>\n<pre>#Shape<br>df.shape<br><br>#Types<br>df.dtypes<br><br>#Head<br>df.head(20)<br><br>#Tail<br>df.tail(20)<br><br>#Describe<br>df.describe().T<br><br>#Total Numbers of Nulls<br>df.isnull().sum()</pre>\n<p>With the methods used above, it can be seen that there are 611107 rows and 26 columns in the data set. In the data set, the number, prices and categories of the products purchased by the customers can be accessed, at the same time, it is possible to view who bought from which store and when, and their personal information.</p>\n<blockquote>Analysis of Categorical and Numerical Variables</blockquote>\n<p>I am trying to understand which is categorical and which is cardinal variable by observing the frequency of the data inside the variables.</p>\n<pre>[print(col, df[col].dtypes,df[col].nunique()) for col in df.columns]</pre>\n<p>Although the types of variables such as clientcode, clientname are object, I did not include them in my categorical data group because their frequencies are very high. I categorically take variables with a maximum frequency of\u00a0360.</p>\n<pre>cat_val = [col for col in df.columns if df[col].dtypes == \"O\" and df[col].nunique() &lt; 360]</pre>\n<p>I have included int and float values with a frequency less than 100 in my numeric but categorical variables.</p>\n<pre>num_but_cat = [col for col in df.columns if df[col].dtype in [int,float] and df[col].nunique() &lt; 100]</pre>\n<pre>cat_val = cat_val + num_but_cat</pre>\n<p>Numerical variables are \"AMOUNT\", \"PRICE\", \"LINENETTOTAL\", \"LINENET\".</p>\n<pre>num_cols = [col for col in [\"AMOUNT\", \"PRICE\", \"LINENETTOTAL\", \"LINENET\"]]</pre>\n<blockquote>Correlation Analysis</blockquote>\n<pre>corr = df[num_cols].corr()<br>corr</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/577/1*zL4ko0L-vm34uJ_qE6UuIw.png\"><figcaption><strong>Correlation Values (We can see there is high correlation between \u201cLINENET\u201d and \u201cLINENETTOTAL\u201d)</strong></figcaption></figure><blockquote><em>Visualization correlation among numerical variables</em></blockquote>\n<pre>sns.set(rc={'figure.figsize': (12, 12)})<br>sns.heatmap(corr, cmap=\"RdBu\")<br>plt.show()</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*hJKd8i-_ichctqe1b_uTwg.png\"></figure><p>or we can use another\u00a0method:</p>\n<pre>pip install klib<br>import klib<br>klib.corr_plot(corr)<br>plt.show()</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*d3M9bJN3DEJ05okQkUS9Jg.png\"></figure><p>For display relation between LINENET and LINENETTOTAL</p>\n<pre>plt.figure(figsize=(10,5))<br>sns.regplot(x=\"LINENET\", y=\"LINENETTOTAL\",data=df)<br>plt.xlim(0,400)<br>plt.ylim(0,400)<br>plt.show()</pre>\n<blockquote>Grouping and aggregation of datas for some\u00a0insights</blockquote>\n<pre><strong>#LINENETTOTAL's sum, mean, min and max values by GENDER</strong></pre>\n<pre>df[[\"LINENETTOTAL\",\"GENDER\"]].groupby(\"GENDER\").agg({\"LINENETTOTAL\":[\"sum\",\"mean\",\"min\", \"max\"]})</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/448/1*FwgT9uc_NJLZou5kPhAFjQ.png\"></figure><pre><strong>#AMOUNT totals by CATEGORY</strong><br><br>df[[\"CATEGORY_NAME1\",\"AMOUNT\"]].groupby([\"CATEGORY_NAME1\"]).agg({\"AMOUNT\":\"sum\"})</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/310/1*hYwjTeoibxj4-BF4LMibmA.png\"></figure><pre><strong>#Top 10 branches with the highest revenue</strong><br><br>df[[\"BRANCH\",\"LINENETTOTAL\"]].groupby([\"BRANCH\"]).agg({\"LINENETTOTAL\":\"sum\"}).sort_values(\"LINENETTOTAL\",ascending = False).head(10)</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/304/1*nHODTWWD29bC-aultIuSTw.png\"></figure><pre><strong>#Top 2 Region with the lowest revenue</strong><br><br>df[[\"REGION\",\"LINENETTOTAL\"]].groupby([\"REGION\"]).agg({\"LINENETTOTAL\":\"sum\"}).sort_values(\"LINENETTOTAL\",ascending = True).head(2)</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/320/1*odEVL2QCn_TnoslRlVQEvg.png\"></figure><pre><strong>#Sales totals and means according to <em>weekdays</em> and <em>weekends</em></strong><br><br>w_df = pd.DataFrame(pd.to_datetime(df['DATE_']))<br>w_df[\"WEEK_VARIABLE\"] = w_df[\"DATE_\"].dt.dayofweek<br>w_df[\"WEEKENDORNOT\"] = [\"WEEKEND\" if week_variable &gt; 4 else \"WEEKDAY\" for week_variable in w_df[\"WEEK_VARIABLE\"]]<br>w_df[\"LINENETTOTAL\"] = df[\"LINENETTOTAL\"]<br><br>w_df[[\"LINENETTOTAL\",\"WEEKENDORNOT\"]].groupby(\"WEEKENDORNOT\").agg({\"LINENETTOTAL\":[\"sum\",\"mean\"]})</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/367/1*CIJqZe4LtqVgQbo6EDLlDQ.png\"></figure><pre><strong>#Top 10 salesman with the highest total</strong><br><br>df[[\"SALESMAN\",\"LINENETTOTAL\"]].groupby([\"SALESMAN\"]).agg({\"LINENETTOTAL\":\"sum\"}).sort_values(\"LINENETTOTAL\",ascending = False).head(10)</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/316/1*vGH1zI5dvyrIAcGeluuP5w.png\"></figure><pre><strong>#Top 10 CLIENT with the highest total</strong><br><br>df[[\"CLIENTNAME\",\"CLIENTCODE\",\"LINENETTOTAL\"]].groupby([\"CLIENTNAME\",\"CLIENTCODE\"]).agg({\"LINENETTOTAL\":\"sum\"}).sort_values(\"LINENETTOTAL\",ascending = False).head(10)</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/480/1*Bwxl-xnpOFKROZ6vRS4_hg.png\"></figure><blockquote>Visualization</blockquote>\n<pre><strong>#Categorical Variables's Visulization</strong><br>for col in cat_val:<br>    sns.countplot(x=df[col], data=df)<br>    plt.show()</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/793/1*Nn8xwoHPfdYA2LgEcS0n1A.png\"></figure><pre><strong>#Distribution of linenettotal by gender</strong><br>sns.boxplot( x=df[\"GENDER\"], y=df[\"LINENETTOTAL\"] )<br>plt.ylim(0,50)<br>plt.show()</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*hzlgaEa8Ehv2LciYcAgL8g.png\"></figure><pre><strong>#View price ranges of categories</strong><br>ax = sns.barplot(x=\"CATEGORY_NAME1\", y=\"PRICE\", data=df)<br>ax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")<br>plt.show()</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*o1Qug7eQZgRcYUJc0vE9fA.png\"></figure><blockquote>Resources:</blockquote>\n<ul>\n<li><a href=\"https://www.python-graph-gallery.com/\">Python Graph Gallery | The Python Graph Gallery</a></li>\n<li><a href=\"https://python.plainenglish.io/lets-speed-up-exploratory-data-analysis-eda-with-the-klib-library-4cf93c81fd12\">Let\u2019s Speed Up Exploratory Data Analysis (EDA) with the Klib Library</a></li>\n<li><a href=\"https://analyticsindiamag.com/guide-to-mito-a-low-code-tool-for-exploratory-data-analysiseda/\">Guide To Mito - A Low Code Tool for Exploratory Data Analysis(EDA)</a></li>\n<li><a href=\"https://www.tutorialsrack.com/articles/324/how-to-find-the-current-day-is-weekday-or-weekends-in-python\">How to Find the Current Day is Weekday or Weekends in Python</a></li>\n<li><a href=\"https://www.kaggle.com/omercolakoglu/turkish-market-sales-dataset-with-9000items\">Turkish Market Sales Dataset With 9.000+Items</a></li>\n<li><a href=\"https://www.veribilimiokulu.com/\">VBO BLOG | Anasayfa</a></li>\n</ul>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=6f284269b195\" width=\"1\" height=\"1\" alt=\"\">\n","enclosure":{},"categories":["pandas","visualization","data-science","python"]}]}